{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34725123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 16:37:45,683 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'o200k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.GPT41SummarizationModel object at 0x7c26a9f13eb0>\n",
      "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x7c26a9f134f0>}\n",
      "            Cluster Embedding Model: OpenAI\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-07-13 16:37:45,684 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'o200k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.GPT41SummarizationModel object at 0x7c26a9f13eb0>\n",
      "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x7c26a9f134f0>}\n",
      "            Cluster Embedding Model: OpenAI\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-07-13 16:37:45,684 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'o200k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: VLLM\n",
      "            Embedding Model: <__main__.VLLMQueryEmbedding object at 0x7c26a9f12980>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "2025-07-13 16:37:45,685 - Async support: True, Batch support: True\n",
      "2025-07-13 16:37:45,685 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'o200k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.GPT41SummarizationModel object at 0x7c26a9f13eb0>\n",
      "            Embedding Models: {'OpenAI': <raptor.EmbeddingModels.OpenAIEmbeddingModel object at 0x7c26a9f134f0>}\n",
      "            Cluster Embedding Model: OpenAI\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'o200k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: VLLM\n",
      "            Embedding Model: <__main__.VLLMQueryEmbedding object at 0x7c26a9f12980>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            Tree Builder Type: cluster\n",
      "        \n",
      "2025-07-13 16:37:45,685 - Using collapsed_tree\n",
      "2025-07-13 16:37:45,731 - Using collapsed_tree\n",
      "2025-07-13 16:37:45,759 - Using collapsed_tree\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running RAPTOR retrieve test...\n",
      "‚úÖ Tree loaded: 65 nodes, 1 layers\n",
      "\n",
      "==================================================\n",
      "üîç RAPTOR RETRIEVAL TEST\n",
      "==================================================\n",
      "\n",
      "üìù Query: Bu dok√ºmanƒ±n ana konusu nedir?\n",
      "----------------------------------------\n",
      "üìÑ Result (1072 chars):\n",
      "Bu metin, \"ZULFICORE SYSTEM\" adlƒ± bir projenin tanƒ±tƒ±mƒ±nƒ± ve yazarƒ±n bu konudaki d√º≈ü√ºncelerini anlatƒ±yor. Yazar, bu devrimi ba≈ülatmaya karar verdiƒüini ve hedefinin d√ºnyada tek bir √ßocuƒüun bile a√ß kalmamasƒ± olduƒüunu ifade ediyor. 18 ya≈üƒ±nda Atat√ºrk‚Äô√º ...\n",
      "\n",
      "üìù Query: En √∂nemli bilgiler nelerdir?\n",
      "----------------------------------------\n",
      "üìÑ Result (884 chars):\n",
      "√ñzet:  Bu metin, √ße≈üitli peygamberlerin kƒ±ssalarƒ±nƒ± ve Zulficore yorumunu ele alarak, mucizelerin ve sessiz yankƒ±larƒ±n kaynaƒüƒ±nƒ± ifade ediyor.   1. **Hz. ƒ∞sa ‚Äì Mucizelerin Kaynaƒüƒ±:** Maide Suresi 110. ayette, Allah, Hz. ƒ∞sa'ya ve annesine verilen nim...\n",
      "\n",
      "üìù Query: What is the main topic?\n",
      "----------------------------------------\n",
      "üìÑ Result (889 chars):\n",
      "I write to you not merely as a technology entrepreneur, but with deep respect for someone who has truly succeeded in uniting science with kindness Your work‚Äîespecially App Inventor and your commitment to open knowledge‚Äîhas brought hope to millions  Y...\n",
      "\n",
      "‚úÖ Retrieval test completed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RAPTOR Retrieve - Works in both terminal and Jupyter\n",
    "\"\"\"\n",
    "import asyncio\n",
    "from raptor import RetrievalAugmentation, RetrievalAugmentationConfig\n",
    "import requests\n",
    "from raptor import BaseEmbeddingModel\n",
    "\n",
    "class VLLMQueryEmbedding(BaseEmbeddingModel):\n",
    "    \"\"\"VLLM embedding for queries with 'query:' prefix\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:8008\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def create_embedding(self, text: str):\n",
    "        # Always use query prefix for retrieval\n",
    "        payload = {\n",
    "            \"input\": [f\"query: {text}\"],\n",
    "            \"model\": \"intfloat/multilingual-e5-large\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{self.base_url}/v1/embeddings\", json=payload, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['data'][0]['embedding']\n",
    "        else:\n",
    "            raise Exception(f\"VLLM error: {response.status_code}\")\n",
    "\n",
    "# =============================================================================\n",
    "# JUPYTER KULLANIMI: A≈üaƒüƒ±daki satƒ±rƒ± kopyala/yapƒ±≈ütƒ±r\n",
    "# await retrieve_test()\n",
    "# =============================================================================\n",
    "\n",
    "async def retrieve_test():\n",
    "    \"\"\"Main retrieval test - USE THIS IN JUPYTER\"\"\"\n",
    "    \n",
    "    tree_path = \"raptor_tree.pkl\"\n",
    "    \n",
    "    # Query embedding model (with query: prefix)\n",
    "    query_embedding_model = VLLMQueryEmbedding()\n",
    "    \n",
    "    # Config for retrieval (must match tree's embedding key)\n",
    "    config = RetrievalAugmentationConfig(\n",
    "        tr_embedding_model=query_embedding_model,\n",
    "        tr_context_embedding_model=\"VLLM\",  # Key from tree\n",
    "        tr_top_k=5,\n",
    "        tr_selection_mode=\"top_k\"\n",
    "    )\n",
    "    \n",
    "    # Load RAPTOR with tree\n",
    "    RA = RetrievalAugmentation(config=config, tree=tree_path)\n",
    "    \n",
    "    print(f\"‚úÖ Tree loaded: {len(RA.tree.all_nodes)} nodes, {RA.tree.num_layers} layers\")\n",
    "    \n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"Bu dok√ºmanƒ±n ana konusu nedir?\",\n",
    "        \"En √∂nemli bilgiler nelerdir?\",\n",
    "        \"What is the main topic?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîç RAPTOR RETRIEVAL TEST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nüìù Query: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Use async retrieve\n",
    "        result = await RA.retrieve_async(\n",
    "            query,              # Direct query (query: prefix automatic)\n",
    "            top_k=3,           # Top 3 nodes\n",
    "            max_tokens=300,    # Max response length\n",
    "            collapse_tree=True # Search all layers\n",
    "        )\n",
    "        \n",
    "        print(f\"üìÑ Result ({len(result)} chars):\")\n",
    "        print(result[:250] + \"...\" if len(result) > 250 else result)\n",
    "    \n",
    "    print(\"\\n‚úÖ Retrieval test completed!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Terminal version only\"\"\"\n",
    "    print(\"üöÄ Running RAPTOR retrieve test...\")\n",
    "    asyncio.run(retrieve_test())\n",
    "\n",
    "# For direct use in Jupyter\n",
    "async def jupyter_test():\n",
    "    \"\"\"Simple single query test\"\"\"\n",
    "    tree_path = \"raptor_tree.pkl\" \n",
    "    query_embedding_model = VLLMQueryEmbedding()\n",
    "    \n",
    "    config = RetrievalAugmentationConfig(\n",
    "        tr_embedding_model=query_embedding_model,\n",
    "        tr_context_embedding_model=\"VLLM\"\n",
    "    )\n",
    "    \n",
    "    RA = RetrievalAugmentation(config=config, tree=tree_path)\n",
    "    print(f\"‚úÖ Tree loaded: {len(RA.tree.all_nodes)} nodes\")\n",
    "    \n",
    "    # Single test query\n",
    "    query = \"Bu dok√ºmanƒ±n ana konusu nedir?\"\n",
    "    result = await RA.retrieve_async(query, top_k=3, max_tokens=300)\n",
    "    \n",
    "    print(f\"\\nüìù Query: {query}\")\n",
    "    print(f\"üìÑ Result: {result[:200]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    print(\"üîç Jupyter kullanƒ±mƒ±:\")\n",
    "    print(\"   await retrieve_test()     # Full test\")\n",
    "    print(\"   await jupyter_test()      # Quick test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
