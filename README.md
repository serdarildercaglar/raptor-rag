# ü¶Ö RAPTOR - Production Optimized RAG System

**RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval**

A production-ready, async-optimized implementation of RAPTOR for high-performance retrieval in agentic AI systems.

## üöÄ Performance Highlights

- **3.3 queries/sec** sustained throughput
- **100% reliability** in concurrent user tests
- **10+ concurrent users** supported
- **1.1x speedup** with batch processing
- **<400ms** average query time

## üéØ What's New in This Version

This is a **production-optimized** version of RAPTOR with the following enhancements:

- ‚úÖ **Async/Await Support** - Full async implementation for non-blocking operations
- ‚úÖ **VLLM Integration** - High-performance embedding service with GPU acceleration 
- ‚úÖ **Batch Processing** - Optimized batch retrieval for agentic AI workflows
- ‚úÖ **GPU Acceleration** - faiss-gpu for faster similarity search
- ‚úÖ **QA Removal** - Streamlined for retrieval-only use cases
- ‚úÖ **Session Management** - Proper resource cleanup and connection pooling
- ‚úÖ **Production Ready** - Tested with 1000+ concurrent operations

## üìã Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [VLLM Embedding Service](#vllm-embedding-service)
- [Usage Examples](#usage-examples)
- [Production Deployment](#production-deployment)
- [API Reference](#api-reference)
- [Performance Benchmarks](#performance-benchmarks)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)

## üõ† Installation

### Prerequisites

- Python 3.8+
- CUDA-compatible GPU (recommended)
- 8GB+ RAM
- 4GB+ VRAM for VLLM service

### 1. Clone the Repository

```bash
git clone https://github.com/your-repo/raptor-production.git
cd raptor-production
```

### 2. Install Dependencies

```bash
# Create virtual environment
python -m venv raptor-env
source raptor-env/bin/activate  # Linux/Mac
# or
raptor-env\Scripts\activate  # Windows

# Install requirements
pip install -r requirements.txt
```

### 3. Install GPU Support (Recommended)

```bash
# For CUDA 11.8+
pip install faiss-gpu

# Verify CUDA support
python -c "import faiss; print(f'GPU count: {faiss.get_num_gpus()}')"
```

## ‚ö° Quick Start

### 1. Start VLLM Embedding Service

```bash
cd embedding_service
chmod +x start_service.sh
./start_service.sh
```

Wait for the message: "Model loaded successfully!"

### 2. Basic Usage

```python
from raptor import RetrievalAugmentation, RetrievalAugmentationConfig, VLLMEmbeddingModel

# Setup
embedding_model = VLLMEmbeddingModel("http://localhost:8008")
config = RetrievalAugmentationConfig(embedding_model=embedding_model)

# Load pre-built RAPTOR tree
RA = RetrievalAugmentation(config=config, tree="path/to/your/raptor_tree.pkl")

# Single query
context = RA.retrieve("What is machine learning?")
print(f"Retrieved context: {len(context)} characters")
```

### 3. Test the Setup

```bash
python test_raptor.py
```

## üî• VLLM Embedding Service

### Configuration

Edit `embedding_service/.env`:

```env
EMBEDDING_MODEL=intfloat/multilingual-e5-large
EMBEDDING_PORT=8008
GPU_MEMORY_UTILIZATION=0.7
EMBEDDING_BATCH_SIZE=32
MAX_BATCH_SIZE=64
MAX_MODEL_LEN=512
```

### Service Management

```bash
# Start service
cd embedding_service
./start_service.sh

# Test service
python test_service.py

# Check health
curl http://localhost:8008/health
```

### Service Endpoints

- **Health Check:** `GET /health`
- **Single Embedding:** `POST /v1/embeddings`
- **Batch Embedding:** `POST /embeddings/batch`

## üí° Usage Examples

### Synchronous Usage (Simple Scripts)

```python
from raptor import RetrievalAugmentation, VLLMEmbeddingModel

# Quick setup
embedding_model = VLLMEmbeddingModel("http://localhost:8008")
config = RetrievalAugmentationConfig(embedding_model=embedding_model)
RA = RetrievalAugmentation(config=config, tree="raptor_tree.pkl")

# Retrieve context
question = "Explain quantum computing"
context = RA.retrieve(question)
print(context)
```

### Asynchronous Usage (Production)

```python
import asyncio
from raptor import RetrievalAugmentation, VLLMEmbeddingModel

async def agentic_ai_workflow():
    embedding_model = VLLMEmbeddingModel("http://localhost:8008")
    
    try:
        config = RetrievalAugmentationConfig(embedding_model=embedding_model)
        RA = RetrievalAugmentation(config=config, tree="raptor_tree.pkl")
        
        # Single async query
        context = await RA.retrieve_async("What is AI?")
        
        # Batch queries (Agentic AI pattern)
        questions = [
            "What is machine learning?",
            "How does deep learning work?", 
            "What are neural networks?"
        ]
        contexts = await RA.retrieve_batch(questions)
        
        return contexts
        
    finally:
        await embedding_model.close()  # Proper cleanup

# Run
results = asyncio.run(agentic_ai_workflow())
```

### Concurrent Users (Production)

```python
import asyncio

async def simulate_user(user_id: int, RA):
    """Simulate one user's queries"""
    questions = [
        f"User {user_id}: What is AI?",
        f"User {user_id}: How does ML work?"
    ]
    
    contexts = await RA.retrieve_batch(questions)
    return len(contexts)

async def concurrent_users():
    embedding_model = VLLMEmbeddingModel("http://localhost:8008")
    
    try:
        config = RetrievalAugmentationConfig(embedding_model=embedding_model)
        RA = RetrievalAugmentation(config=config, tree="raptor_tree.pkl")
        
        # Simulate 10 concurrent users
        tasks = [simulate_user(i, RA) for i in range(10)]
        results = await asyncio.gather(*tasks)
        
        print(f"Processed {sum(results)} queries for {len(results)} users")
        
    finally:
        await embedding_model.close()

asyncio.run(concurrent_users())
```

## üè≠ Production Deployment

### 1. Environment Setup

```bash
# Production environment variables
export RAPTOR_TREE_PATH="/path/to/production/tree.pkl"
export VLLM_SERVICE_URL="http://localhost:8008"
export RAPTOR_LOG_LEVEL="INFO"
```

### 2. Production Configuration

```python
def create_production_config():
    return RetrievalAugmentationConfig(
        embedding_model=VLLMEmbeddingModel(
            base_url=os.environ.get("VLLM_SERVICE_URL", "http://localhost:8008"),
            timeout=10
        ),
        tr_top_k=5,              # Optimized for speed
        tr_threshold=0.6,        # Higher precision
        tr_selection_mode="top_k"
    )
```

### 3. Production Patterns

#### Agentic AI Integration

```python
class AgenticRAGService:
    def __init__(self, tree_path: str):
        self.embedding_model = VLLMEmbeddingModel()
        config = RetrievalAugmentationConfig(embedding_model=self.embedding_model)
        self.RA = RetrievalAugmentation(config=config, tree=tree_path)
    
    async def process_user_request(self, user_query: str) -> dict:
        """Process a user request with multiple sub-queries"""
        
        # Agentic AI generates sub-queries
        sub_queries = await self.generate_sub_queries(user_query)
        
        # Batch retrieve contexts
        contexts = await self.RA.retrieve_batch(sub_queries)
        
        # Return structured response
        return {
            "original_query": user_query,
            "sub_queries": sub_queries,
            "contexts": contexts,
            "total_context_length": sum(len(ctx) for ctx in contexts)
        }
    
    async def generate_sub_queries(self, query: str) -> List[str]:
        """Generate 3-5 sub-queries for better retrieval"""
        # Your agentic AI logic here
        return [
            f"What is {query}?",
            f"How does {query} work?", 
            f"Examples of {query}",
        ]
    
    async def cleanup(self):
        await self.embedding_model.close()
```

#### FastAPI Service

```python
from fastapi import FastAPI, HTTPException
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    app.state.rag_service = AgenticRAGService("raptor_tree.pkl")
    yield
    # Shutdown
    await app.state.rag_service.cleanup()

app = FastAPI(lifespan=lifespan)

@app.post("/retrieve")
async def retrieve_endpoint(query: str):
    try:
        result = await app.state.rag_service.process_user_request(query)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## üìñ API Reference

### RetrievalAugmentation

#### Methods

- `retrieve(question: str, **kwargs) -> str`
  - Synchronous retrieval (avoid in async contexts)
  
- `retrieve_async(question: str, **kwargs) -> str`
  - Async retrieval for single question
  
- `retrieve_batch(questions: List[str], **kwargs) -> List[str]`
  - Optimized batch retrieval

#### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `question(s)` | str/List[str] | - | Query or list of queries |
| `top_k` | int | 10 | Number of top results |
| `max_tokens` | int | 3500 | Maximum context tokens |
| `collapse_tree` | bool | True | Use collapsed tree search |
| `return_layer_information` | bool | False | Include layer metadata |

### VLLMEmbeddingModel

```python
class VLLMEmbeddingModel:
    def __init__(self, base_url: str = "http://localhost:8008", timeout: int = 30)
    
    # Sync methods
    def create_embedding(self, text: str) -> List[float]
    
    # Async methods
    async def create_embedding_async(self, text: str) -> List[float]
    async def create_embeddings_batch(self, texts: List[str]) -> List[List[float]]
    async def close(self)  # Always call in production
```

## üìä Performance Benchmarks

### Test Environment
- **GPU:** NVIDIA RTX 3090 24GB
- **CPU:** AMD Ryzen 9 5900X
- **RAM:** 32GB DDR4
- **Storage:** NVMe SSD

### Benchmark Results

| Test Scenario | Performance | Details |
|---------------|-------------|---------|
| **Single Query** | 375ms avg | Individual retrieve_async calls |
| **Batch (4 queries)** | 355ms avg | 1.1x speedup vs individual |
| **Concurrent Users** | 3.3 queries/sec | 10 users simultaneously |
| **Sustained Load** | 2.6 requests/sec | 30-second continuous test |
| **Success Rate** | 100% | Zero failures in all tests |

### Performance by Query Type

```python
# Performance test results
{
    "simple_factual": "250-300ms",
    "complex_analytical": "400-500ms", 
    "multi_topic": "500-600ms",
    "batch_processing": "90-120ms per query"
}
```

### Scaling Characteristics

```
Users    | Throughput | Avg Response
---------|------------|-------------
1        | 2.7 q/s    | 370ms
5        | 3.1 q/s    | 1.6s
10       | 3.3 q/s    | 3.4s
20       | 2.8 q/s    | 7.1s (degradation)
```

## üîß Troubleshooting

### Common Issues

#### VLLM Service Not Starting

```bash
# Check GPU availability
nvidia-smi

# Check port availability  
lsof -i :8008

# Check logs
cd embedding_service
python server.py
```

#### Memory Issues

```bash
# Reduce GPU memory usage
# Edit embedding_service/.env
GPU_MEMORY_UTILIZATION=0.5
EMBEDDING_BATCH_SIZE=16
```

#### Connection Errors

```python
# Check service health
import requests
response = requests.get("http://localhost:8008/health")
print(response.json())
```

#### Performance Issues

```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Monitor performance
import time
start = time.time()
context = await RA.retrieve_async("test query")
print(f"Query time: {time.time() - start:.2f}s")
```

### FAQ

**Q: Can I use this without GPU?**
A: Yes, but performance will be significantly slower. Set `use_gpu=False` in configs.

**Q: How do I handle session cleanup in production?**
A: Always use async context managers or try/finally blocks with `await embedding_model.close()`.

**Q: What's the recommended batch size?**
A: 3-5 queries per batch for optimal performance. Larger batches may not improve speed significantly.

**Q: How do I monitor production performance?**
A: Log query times, implement health checks, and monitor GPU memory usage.

## üõ† Development

### Running Tests

```bash
# Unit tests
python -m pytest tests/

# Integration tests
python test_integration.py

# Performance tests
python test_production_performance.py
```

### Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Original RAPTOR paper: [Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059)
- VLLM project for high-performance inference
- Faiss for efficient similarity search

## üìû Support

- **Issues:** [GitHub Issues](https://github.com/your-repo/raptor-production/issues)
- **Discussions:** [GitHub Discussions](https://github.com/your-repo/raptor-production/discussions)
- **Documentation:** [Wiki](https://github.com/your-repo/raptor-production/wiki)

---

**Ready for production? Start with the [Quick Start](#quick-start) guide!** üöÄ